{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Kavya Reddy Gondhi\n",
        "\n",
        "Week 09 - Machine Learning with Scikit-learn\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pWeP51hTdduv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this week’s assignment, you are required to investigate the accuracy-computation time tradeoffs of the different optimization algorithms (solvers) that are available for fitting linear regression models in Scikit-Learn. Using the code shared via the Python notebook (part of this week’s uploads archive) where the use of logistic regression was demonstrated, complete the following operations:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "1. Among the different classification models included in the Python notebook, which model had the best overall performance? Support your response by referencing appropriate evidence.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "The Logistic Regression with L1 penalty and C=10 parameter (named 'Logistic_L1_C_10') proved to be the most effective classification model according to the notebook data. Testing results indicated the model reached 0.718 accuracy while the training results achieved 0.7347 accuracy. The scores demonstrate the best balanced performance between training and testing datasets when compared to all tested models.\n",
        "The final results summary table provides evidence which validates this conclusion. The Logistic_L1_C_10 model demonstrated superior performance than both the basic Logistic Regression model and the Null model and other L1 penalty variations with different C values. The RandomForest_noCV model displayed outstanding 0.9993 training accuracy yet its test accuracy reached only 0.686 because of severe overfitting.\n",
        "The analysis proves that classification problems need balanced regularization for optimal results. The L1 penalty (LASSO) reduces coefficient values toward zero which enables it to select important features by removing weak predictors while keeping strong predictors. The C=10 value in the penalty function enables the model to maintain numerous features while avoiding overfitting.\n",
        "The predictive accuracy improvement of the Logistic_L1_C_10 model reaches 9 percentage points over the null model (0.6467 training, 0.608 testing) results. The regularized logistic regression model demonstrated superior generalization ability to unseen data points compared to the random forest models when applied to this particular dataset."
      ],
      "metadata": {
        "id": "RQcSlhQbdjRm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DrVLPgogYJAV"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "from patsy import dmatrices\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_patient = pd.read_csv('./PatientAnalyticFile.csv')\n",
        "\n",
        "data_patient['mortality'] = np.where(data_patient['DateOfDeath'].isnull(), 0, 1)\n",
        "\n",
        "data_patient['DateOfBirth'] = pd.to_datetime(data_patient['DateOfBirth'])\n",
        "data_patient['Age_years'] = ((pd.to_datetime('2015-01-01') - data_patient['DateOfBirth']).dt.days/365.25)\n",
        "\n",
        "vars_remove = ['PatientID', 'First_Appointment_Date', 'DateOfBirth',\n",
        "               'Last_Appointment_Date', 'DateOfDeath', 'mortality']\n",
        "vars_left = set(data_patient.columns) - set(vars_remove)\n",
        "formula = \"mortality ~ \" + \" + \".join(vars_left)"
      ],
      "metadata": {
        "id": "OcJsenp2eL72"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y, X = dmatrices(formula, data_patient)\n",
        "\n",
        "# Splitting the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, np.ravel(Y), test_size=0.2, random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "VRtfrXU0eWzR"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "solvers = ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
        "results = []"
      ],
      "metadata": {
        "id": "pVrbAWb5ebAK"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for solver in solvers:\n",
        "    start_time = time.time()\n",
        "\n",
        "    model = LogisticRegression(solver=solver, max_iter=1000, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    train_accuracy = accuracy_score(y_train, model.predict(X_train))\n",
        "    test_accuracy = accuracy_score(y_test, model.predict(X_test))\n",
        "\n",
        "    execution_time = time.time() - start_time\n",
        "\n",
        "    results.append({\n",
        "        'Solver': solver,\n",
        "        'Training Accuracy': round(train_accuracy, 4),\n",
        "        'Holdout Accuracy': round(test_accuracy, 4),\n",
        "        'Time Taken (seconds)': round(execution_time, 4)\n",
        "    })\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "print(results_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u2EtsF5Aedn9",
        "outputId": "aa743830-c203-459b-9627-4cee6ad900b8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Solver  Training Accuracy  Holdout Accuracy  Time Taken (seconds)\n",
            "0  newton-cg             0.7482            0.7362                0.1521\n",
            "1      lbfgs             0.7481            0.7355                2.0240\n",
            "2  liblinear             0.7479            0.7362                0.1490\n",
            "3        sag             0.7481            0.7362               18.0974\n",
            "4       saga             0.7480            0.7362               11.9205\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Among all logistic regression solvers liblinear demonstrates the best performance regarding accuracy and execution time combination. All solvers demonstrated comparable accuracy results through training accuracy reaching 0.748 and holdout accuracy reaching 0.736 yet they displayed notable differences in execution time.\n",
        "The liblinear solver completed its execution in 0.149 seconds without affecting holdout accuracy which remained at 0.7362 along with newton-cg, sag, and saga. The execution time for newton-cg at 0.1521 seconds was comparable to liblinear which completed slightly faster to achieve equivalent holdout results.\n",
        "The holdout accuracy reached identical levels among newton-cg, liblinear, sag, and saga at 0.7362 but lbfgs achieved a slightly lower score at 0.7355. For this dataset the selection of solver does not affect model performance to a meaningful extent based on accuracy measurements.\n",
        "The solvers exhibited major variations in their execution times. The execution time of sag and saga solvers reached 18.10 and 11.92 seconds which demonstrated approximately 80-120 times slower performance compared to liblinear. The lbfgs solver ran at an intermediate pace of 2.02 seconds yet remained slower by more than 13 times compared to liblinear. Several solvers displayed convergence warning messages because they stopped before reaching a complete convergence point during their maximum iteration period.\n",
        "Liblinear proves to be the best option for practical applications due to its speed advantage when its accuracy matches slower alternatives which occurs in this situation."
      ],
      "metadata": {
        "id": "HXU3jxh5fArb"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nn3i2ZEre6Io"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}